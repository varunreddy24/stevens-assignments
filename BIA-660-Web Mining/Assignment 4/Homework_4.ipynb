{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>HW 4: Text preprocessing</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, json, string\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.tokenizer import Tokenizer\n",
    "import spacy\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1: Regular Expression (2 points)\n",
    "\n",
    "Suppose you have scraped the text shown below from an online source. You'd like to extract data using regular expression.\n",
    "\n",
    "Define a **extract** function which:\n",
    "- Takes a piece of text (in the format of shown below) as an input\n",
    "- Extracts data into a list of tuples using regular expression, e.g.  `[('BTC-USD','56,212.15','-58.16','-0.10%'), ('ETH-USD',  ...), ...]`\n",
    "- Returns the list of tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Symbol   Last Price  Change   % Change   Note\\n                  BTC-USD  56,212.15   -58.16   -0.10%   Bitcoin\\n                  ETH-USD  1,787.79    -53.63   -2.91%   Ether\\n                  BNB-USD  1,101,290.51      +51.81    +2.04%   Binance\\n                  USDT-USD 1.0003      -0.0004  -0.04%   Tether\\n                  ADA-USD  1.1187      -0.0528  -4.51%   Cardano\\n      '"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text='''Symbol   Last Price  Change   % Change   Note\n",
    "                  BTC-USD  56,212.15   -58.16   -0.10%   Bitcoin\n",
    "                  ETH-USD  1,787.79    -53.63   -2.91%   Ether\n",
    "                  BNB-USD  1,101,290.51      +51.81    +2.04%   Binance\n",
    "                  USDT-USD 1.0003      -0.0004  -0.04%   Tether\n",
    "                  ADA-USD  1.1187      -0.0528  -4.51%   Cardano\n",
    "      '''\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "def extract(text):\n",
    "    text = re.sub(\" +\",\" \", text)\n",
    "    regexPattern = re.compile(r\"([A-Z]{2,4}[-][A-Z]{3}) (\\b[\\d,]+\\.?\\d*\\b) ([\\+\\-]\\b[\\d,]+\\.?\\d*\\b) ([\\+\\-]\\b[\\d,]+\\.?\\d*\\b\\%)\")\n",
    "    return regexPattern.findall(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('BTC-USD', '56,212.15', '-58.16', '-0.10%'),\n",
       " ('ETH-USD', '1,787.79', '-53.63', '-2.91%'),\n",
       " ('BNB-USD', '1,101,290.51', '+51.81', '+2.04%'),\n",
       " ('USDT-USD', '1.0003', '-0.0004', '-0.04%'),\n",
       " ('ADA-USD', '1.1187', '-0.0528', '-4.51%')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function\n",
    "\n",
    "extract(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2: Collocation (3 points)\n",
    "\n",
    "Define a function `top_collocation(doc, K)` to find top-K collocations in specific patterns in a document as follows:\n",
    "  - Takes a document (i.e. `doc`) and `K` as inputs\n",
    "  - Find collocations as follows:\n",
    "    - Tokenize the document and find POS tag of each token (hint: you can use NLTK word tokenizer or Spacy tokenizer).\n",
    "    - Create bigrams from the tokens with POS tags.\n",
    "\n",
    "    - Keep only bigrams matching the following patterns:\n",
    "       - `Adj + Noun`: e.g. linear function\n",
    "       - `Noun + Noun`: e.g. regression coefficient\n",
    "    - Get frequency of each bigram (hint: you can use nltk.FreqDist)\n",
    "    - Returns top K collocations by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "\n",
    "def top_collocation(doc, K):\n",
    "\n",
    "    doc = re.sub(\"\\n+\", \" \", doc)\n",
    "    tokenizer = spacy.load('en_core_web_sm')\n",
    "    tokens = tokenizer(doc)\n",
    "    tokens = [i for i in tokens if i.is_stop == False]\n",
    "    # print(dir(tokens[0]))\n",
    "    collocationDict = {}\n",
    "    for i in range(1, len(tokens)):\n",
    "        pairItemsText = (tokens[i-1].text, tokens[i].text)\n",
    "        pairItemsTag = (tokens[i-1].pos_, tokens[i].pos_)\n",
    "        if pairItemsText in collocationDict:\n",
    "            collocationDict[pairItemsText] += 1\n",
    "        elif pairItemsTag[0] in ['ADJ', 'NOUN','PROPN'] and pairItemsTag[1] in ['NOUN', 'PROPN']:\n",
    "            collocationDict[pairItemsText] = 1\n",
    "    \n",
    "    return sorted(collocationDict.items(), key=lambda x: x[1], reverse=True)[:K]\n",
    "        \n",
    "    \n",
    "    # return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('public', 'health'), 14),\n",
       " (('community', 'spread'), 9),\n",
       " (('United', 'States'), 8),\n",
       " (('risk', 'exposure'), 5),\n",
       " (('spread', 'COVID-19'), 4),\n",
       " (('higher', 'risk'), 4),\n",
       " (('COVID-19', 'illness'), 4),\n",
       " (('spread', 'virus'), 4),\n",
       " (('elevated', 'risk'), 4),\n",
       " (('health', 'threat'), 3)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = json.load(open(\"qa.json\",\"r\"))\n",
    "article = data[\"context\"]\n",
    "\n",
    "top_collocation(article, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3: Question and Answering (QA) System (5 points)\n",
    "\n",
    "Develop a QA system which allow you to search for answers in an article. For example, the file `qa.json` contains a research article. This article can answer a number of questions about COVID-19. You will design a solution to automatically search answers to these questions in this article.\n",
    "\n",
    "`qa.json` is taken from https://github.com/deepset-ai/COVID-QA. This file contains a few questions, and answers to these questions have been located in the article. Let's define a QA system and check if your system can locate the right answers.\n",
    "\n",
    "The following script helps you understand `qa.json`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDC Summary 21 MAR 2020,\n",
      "https://www.cdc.gov/coronavirus/2019-ncov/cases-updates/summary.html\n",
      "\n",
      "This is a rapidly evolving situation and CDC will provide updated information and guidance as it becomes \n"
     ]
    }
   ],
   "source": [
    "# Retrieve the article\n",
    "\n",
    "data = json.load(open(\"qa.json\",\"r\"))\n",
    "article = data[\"context\"]\n",
    "\n",
    "# A long article. Just print the first 200 characters\n",
    "print(article[0:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': 'What age group has the highest rate of severe outcomes?', 'id': 236, 'answers': [{'text': 'people 85 years and older', 'answer_start': 6117}], 'is_impossible': False}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['What age group has the highest rate of severe outcomes?',\n",
       " 'How is COVID-19 spread?',\n",
       " 'How many states in the U.S. have reported cases of COVID-19?',\n",
       " 'When did the White House launch the \"15 Days to Slow the Spread\" program?',\n",
       " 'What should mildly-ill patients do?',\n",
       " 'What type of virus is SARS-CoV-2?',\n",
       " 'What viruses are similar to the COVID-19 coronavirus?',\n",
       " 'What are the phases of a pandemic?',\n",
       " 'At which phase does the peak of the pandemic occur?',\n",
       " 'People with which medical conditions have a higher rate of severe illness?',\n",
       " 'What kind of test can diagnose COVID-19?',\n",
       " 'In what species did the COVID-19 virus likely originate?',\n",
       " 'What risk factors should be considered in addition to clinical symptoms?']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve all the questions and answers\n",
    "qas = data[\"qas\"]\n",
    "\n",
    "# show the first question-answer pair. Note the answer starts at the 6117th character\n",
    "print(qas[0])\n",
    "\n",
    "# get all questions\n",
    "qs = [item[\"question\"] for item in qas]\n",
    "qs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, following the instructions below step by step to develop the QA system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.1. Tokenizer\n",
    "\n",
    "Define a function `tokenize(doc)`  as follows:\n",
    "   - Take a piece of text (i.e. variable `doc`) as an input\n",
    "   - Split the input text into unigrams\n",
    "   - Clean up tokens as follows:\n",
    "       - Lemmatize all unigrams\n",
    "       - Remove all stop words\n",
    "       - Remove all punctuations\n",
    "       - Convert all unigrams to the lower case \n",
    "       - remove empty unigrams\n",
    "   - Return the list of unigrams after all the processing. (Hint: you can use spacy package for this task. To test if a token is stop word or punctuation, check https://spacy.io/api/token#attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "\n",
    "\n",
    "def tokenize(doc):    \n",
    "    # add your code\n",
    "\n",
    "    tokenizer = spacy.load('en_core_web_sm')\n",
    "    tokens = tokenizer(doc)\n",
    "\n",
    "    tokens = [str.lower(token.lemma_) for token in tokens if not (token.is_stop or token.is_punct or token.text == '')]                  \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['old', 'people', 'people', 'age', 'severe', 'chronic', 'medical', 'condition', 'like', 'heart', 'disease', 'lung', 'disease', 'diabetes', 'example', 'high', 'risk', 'develop', 'covid-19', 'illness']\n"
     ]
    }
   ],
   "source": [
    "doc = 'Older people and people of all ages with severe chronic medical conditions — \\\n",
    "like heart disease, lung disease and diabetes, \\\n",
    "for example — seem to be at higher risk of developing serious COVID-19 illness.'\n",
    "\n",
    "print(tokenize(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2. Compute TF-IDF Matrix\n",
    "\n",
    "Define a function `compute_tfidf(docs)` as follows: \n",
    "\n",
    "- Take `docs`, a list of documents (e.g. a list of questions) as an input\n",
    "- Tokenize each document in `docs` using the `tokenize` function defined in Q3.1. \n",
    "- Calculate tf_idf weights as shown in lecture notes (Hint: you can reuse the last code segment in NLP Lecture Notes (II))\n",
    "- Return a smoothed normalized `tf_idf` array. (The result may differ a little bit depending on the tokenize function and packages you use.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function\n",
    "def get_doc_tokens(doc):\n",
    "    tokens = tokenize(doc)\n",
    "    tokensCount = {token: tokens.count(token) for token in set(tokens)}\n",
    "    return tokensCount\n",
    "\n",
    "def get_doc_term_matrix(docs):\n",
    "    docs_token = {idx: get_doc_tokens(doc) for idx, doc in enumerate(docs)}\n",
    "    dtm = pd.DataFrame.from_dict(docs_token, orient=\"index\")\n",
    "    dtm = dtm.fillna(0)\n",
    "    dtm = dtm.sort_index(axis=0)\n",
    "    return dtm\n",
    "\n",
    "def get_tf(docs):\n",
    "    tf = get_doc_term_matrix(docs)\n",
    "    doc_len = tf.sum(axis=1)\n",
    "    tf = np.divide(tf.T, doc_len).T\n",
    "    return tf\n",
    "    \n",
    "def get_idf(docs):\n",
    "    df = get_tf(docs)\n",
    "    df = np.where(df>0, 1, 0)\n",
    "    idf = np.log(np.divide(len(docs), np.sum(df, axis=0)))+1\n",
    "    idf = np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1\n",
    "    return idf\n",
    "\n",
    "def compute_tfidf(docs):\n",
    "    tf = get_tf(docs)\n",
    "    idf = get_idf(docs)\n",
    "    tf_idf = normalize(tf*idf)\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.41, 0.41, 0.41, 0.41, 0.41, 0.41, 0.  , 0.  , 0.  , 0.  , 0.  ,\n",
       "        0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.61, 0.8 , 0.  , 0.  , 0.  ,\n",
       "        0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.36, 0.  , 0.47, 0.47, 0.47,\n",
       "        0.47]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function using three questions\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "compute_tfidf(qs[0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3.2. Put Everything Together\n",
    "\n",
    "Define a function `find_solutions(qs, article)` as follows: \n",
    "\n",
    "- Take two inputs:\n",
    "    - `qs`: a list of questions (i.e. strings)\n",
    "    - `article`: a document which may contain answers to the questions\n",
    "- Segment the article into sentences (i.e. `sents`). You will locate the sentence which can answer a question.\n",
    "- Concatenate the questions (`qs`) and sentences (`sents`) into a single list (i.e. `qs + sents`)\n",
    "- Call the function `compute_tfidf` defined in Q3.2 with `qs + sents` to get a `TF-IDF` matrix. (Note, now `qs` and `sents` are converted to TF-IDF vectors in the same dimension. As a result, you can measure their similarities.) \n",
    "- Split the `TF-IDF` matrix into two sub matrices, one corresponding to `qs` and the other for `sents`. \n",
    "- Next, calculate the pairwise cosine similarity between the `qs` and `sents`. With $m$ questions and $n$ sentences, you should get a $m \\times n$ matrix. (hint: you can `sklearn.metrics.pairwise_distances` to calculate pairwise distances between two matrices)\n",
    "- Finally, the answer to each question is the sentence which has the `maximum similarity` to it. \n",
    "- Print out each question and its matched answer. Check if your QA system is able to find the right answer.(Depending on the packages you use, your answer might be a bit different from mine.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_solutions(qs, article):\n",
    "    tokenizer = spacy.load('en_core_web_sm')\n",
    "    tokens = tokenizer(article)\n",
    "    sents = [i.text.strip() for i in tokens.sents]\n",
    "    \n",
    "    tfIdf = compute_tfidf(qs+sents)\n",
    "\n",
    "    qsVector = tfIdf[:len(qs)]\n",
    "    sentsVector = tfIdf[len(qs):]\n",
    "#     print(tfIdf.shape)\n",
    "    # print(qsVector.shape)\n",
    "    # print(sentsVector.shape)\n",
    "    dist = 1-pairwise_distances(qsVector, sentsVector, metric='cosine')\n",
    "    solutions = []\n",
    "\n",
    "    for i in range(len(qs)):\n",
    "        sentInd = np.argmax(dist[i])\n",
    "        solutions.append([\"Question: %s\"%qs[i], \"Answer: %s\"%sents[sentInd]])\n",
    "    \n",
    "    return solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Question: What age group has the highest rate of severe outcomes?',\n",
       "  'Answer: Reported illnesses have ranged from very mild (including some with no reported symptoms) to severe, including illness resulting in death.'],\n",
       " ['Question: How is COVID-19 spread?',\n",
       "  'Answer: If you have been in China or another affected area or have been exposed to someone sick with COVID-19 in the last 14 days, you will face some limitations on your movement and activity.'],\n",
       " ['Question: How many states in the U.S. have reported cases of COVID-19?',\n",
       "  'Answer: All 50 states have reported cases of COVID-19 to CDC.'],\n",
       " ['Question: When did the White House launch the \"15 Days to Slow the Spread\" program?',\n",
       "  'Answer: CDC Recommends\\nEveryone can do their part to help us respond to this emerging public health threat:\\nOn March 16, the White House announced a program called “15 Days to Slow the Spread,”pdf iconexternal icon which is a nationwide effort to slow the spread of COVID-19 through the implementation of social distancing at all levels of society.'],\n",
       " ['Question: What should mildly-ill patients do?',\n",
       "  'Answer: Most people have mild illness and are able to recover at home without medical care.'],\n",
       " ['Question: What type of virus is SARS-CoV-2?',\n",
       "  'Answer: Emergence\\nCOVID-19 is caused by a coronavirus.'],\n",
       " ['Question: What viruses are similar to the COVID-19 coronavirus?',\n",
       "  'Answer: Please follow instructions during this time.'],\n",
       " ['Question: What are the phases of a pandemic?',\n",
       "  'Answer: Older people and people of all ages with severe chronic medical conditions — like heart disease, lung disease and diabetes, for example — seem to be at higher risk of developing serious COVID-19 illness.'],\n",
       " ['Question: At which phase does the peak of the pandemic occur?',\n",
       "  'Answer: As a result, most research and guidance around pandemics is specific to influenza, but the same premises can be applied to the current COVID-19 pandemic.'],\n",
       " ['Question: People with which medical conditions have a higher rate of severe illness?',\n",
       "  'Answer: Older people and people with severe chronic conditions should take special precautions because they are at higher risk of developing serious COVID-19 illness.'],\n",
       " ['Question: What kind of test can diagnose COVID-19?',\n",
       "  'Answer: CDC also has issued guidance for other settings, including:\\nPreparing for COVID-19: Long-term Care Facilities, Nursing Homes\\nDiscontinuation of Home Isolation for Persons with COVID-19\\nCDC has deployed multidisciplinary teams to support state health departments in case identification, contact tracing, clinical management, and public communications.'],\n",
       " ['Question: In what species did the COVID-19 virus likely originate?',\n",
       "  'Answer: Your cooperation is integral to the ongoing public health response to try to slow spread of this virus.'],\n",
       " ['Question: What risk factors should be considered in addition to clinical symptoms?',\n",
       "  'Answer: If you are a healthcare provider, use your judgment to determine if a patient has signs and symptoms compatible with COVID-19 and whether the patient should be tested.']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the system\n",
    "np.set_printoptions(precision=10)\n",
    "find_solutions(qs, article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is COVID-19 spread?\n",
      "\n",
      "U.S. COVID-19 cases include:\n",
      "Imported cases in travelers\n",
      "Cases among close contacts of a known case\n",
      "Community-acquired cases where the source of the infection is unknown.\n"
     ]
    }
   ],
   "source": [
    "print(qs[1])\n",
    "token = tokenize(article)\n",
    "en = spacy.load('en_core_web_sm')\n",
    "\n",
    "print([s for s in en(article).sents][len(qs)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
